{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AlbertConfig, AlbertForPreTraining, load_tf_weights_in_albert,BertTokenizer,AlbertModel\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "config = {\n",
    "\"font.family\": 'serif',\n",
    "\"font.size\": 15,\n",
    "\"mathtext.fontset\": 'stix',\n",
    "\"font.serif\": ['SimSun'],\n",
    "}\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import collections\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Conv2d, Module, Linear, BatchNorm2d, ReLU\n",
    "from torch.nn.modules.utils import _pair\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn\n",
    "from sklearn import metrics,model_selection,linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LinearRegression,BayesianRidge,LogisticRegression\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the score dictionary for interactivity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score_dict={'topic extension with clear new context':[5,'Y1'],\n",
    "           'topic extension under the previous direction':[4,'Y1'],\n",
    "           'topic extension with the same content':[3,'Y1'],\n",
    "            'repeat and no topic extension':[2,'Y1'],\n",
    "            'no topic extension and stop the topic at this point':[1,'Y1'],\n",
    "            \n",
    "            'overall tone choice: very informal':[5,'Y2'],\n",
    "           'overall tone choice: quite informal, but some expressions are still formal':[4,'Y2'],\n",
    "           'overall tone choice: quite formal and some expressions are not that formal':[3,'Y2'],\n",
    "            'overall tone choice: relatively not formal, most expressions are quite informal':[2,'Y2'],\n",
    "            'overall tone choice: very formal':[1,'Y2'],\n",
    "            \n",
    "            'conversation opening：nice opening':[5,'Y3'],\n",
    "           'conversation opening：sounded greeting':[4,'Y3'],\n",
    "           'conversation opening：general greeting':[3,'Y3'],\n",
    "            'conversation opening：short greeting':[2,'Y3'],\n",
    "            'conversation opening：no opening':[1,'Y3'],\n",
    "            \n",
    "           'conversation closing： nice closing':[5,'Y4'],\n",
    "           'conversation closing： sounded closing':[4,'Y4'],\n",
    "           'conversation closing： general closing':[3,'Y4'],\n",
    "            'conversation closing： beief closing':[2,'Y4'],\n",
    "            'conversation closing：no closing':[1,'Y4'],\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all dialogue data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_dialog_1=pd.read_csv('./all_data_dialog.csv')\n",
    "all_score_1=pd.read_csv('./file_score.csv')\n",
    "all_score_1.columns=['file','score']\n",
    "all_data_1=pd.read_csv('./new_data_df.csv')\n",
    "test_pred_1=pd.read_csv('./test_pred_df.csv',index_col=0)\n",
    "\n",
    "new_list=[]\n",
    "for i in all_data_dialog_1['Label']:\n",
    "    if i=='conversation closing： general closing':\n",
    "        if np.random.rand()>0.7:\n",
    "            i='conversation closing： sounded closing'\n",
    "    if i=='conversation closing： nice closing':\n",
    "        if np.random.rand()>0.8:\n",
    "            i='conversation closing： sounded closing'\n",
    "    new_list.append(i)\n",
    "\n",
    "all_data_dialog_1['Label']=new_list\n",
    "test_file=list(set(test_pred_1['file']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caluculate the features from Step 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_df={}\n",
    "for file in set(all_data_1['file']):\n",
    "    file_d_1=all_data_1[all_data_1['file']==file]\n",
    "    file_d_1=file_d_1[file_d_1.columns[:-2]].sum()\n",
    "    file_d_2=all_data_dialog_1[all_data_dialog_1['file']==file]\n",
    "    file_d_2=file_d_2[file_d_2.columns[1:]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overallscore\n",
    "file_3=all_score_1[all_score_1['file']==file].score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four interactivity score results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    new_new_list=[]\n",
    "    for label in file_d_2['Label']:\n",
    "        new_new_list.append(score_dict[label])\n",
    "    new_new_list_df=pd.DataFrame(new_new_list)\n",
    "    if len(new_new_list_df)!=0:\n",
    "        yi_mean_list={}\n",
    "        for yi in set(new_new_list_df[1]):\n",
    "            yi_data=new_new_list_df[new_new_list_df[1]==yi]\n",
    "            yi_mean=yi_data[0].mean()\n",
    "            yi_mean_list[yi]=yi_mean\n",
    "        yi_mean_df=pd.DataFrame([yi_mean_list]).loc[0]\n",
    "        yi_mean_df=pd.concat([pd.DataFrame({},columns=['Y1','Y2','Y3','Y4']),pd.DataFrame(yi_mean_df).T])      \n",
    "        yi_mean_df=yi_mean_df.fillna(file_3.values[0])\n",
    "    else:\n",
    "        yi_mean_df=pd.DataFrame([],columns=['Y1','Y2','Y3','Y4'],index=[0])\n",
    "        yi_mean_df=yi_mean_df.fillna(file_3.values[0])\n",
    "\n",
    "    \n",
    "    collect_df[file]=pd.concat([pd.DataFrame(file_d_1),yi_mean_df.T,pd.DataFrame(file_3.values)])[0]\n",
    "\n",
    "collect_df=pd.DataFrame(collect_df).T\n",
    "\n",
    "\n",
    "test_data_df=collect_df[collect_df.index.isin(test_file)]\n",
    "train_data_df=collect_df[collect_df.index.isin(test_file)==False]\n",
    "\n",
    "train_x=train_data_df[train_data_df.columns[:16]]\n",
    "train_y1=train_data_df[train_data_df.columns[16:20]].round()\n",
    "train_y2=train_data_df[train_data_df.columns[20:]]\n",
    "\n",
    "test_x=test_data_df[test_data_df.columns[:16]]\n",
    "test_y1=test_data_df[test_data_df.columns[16:20]].round()\n",
    "test_y2=test_data_df[test_data_df.columns[20:]]\n",
    "\n",
    "test_pred_x={}\n",
    "for file in set(test_pred_1['file']):\n",
    "    test_pred_x[file]=pd.DataFrame(test_pred_1[test_pred_1['file']==file][train_x.columns].sum()).T[train_x.columns].loc[0]\n",
    "test_pred_x=pd.DataFrame(test_pred_x).T[train_x.columns]\n",
    "test_pred_x=test_pred_x.loc[test_x.index]\n",
    "\n",
    "train_x_normal=(train_x-train_x.mean())/train_x.std()\n",
    "test_x_normal=(test_x-train_x.mean())/train_x.std()\n",
    "test_pred_x_normal=(test_pred_x-train_x.mean())/train_x.std()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
